{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_legends.preprocess import load_data, transform, majority, holdout_majority, load_test\n",
    "import polars as pl\n",
    "#deberta\n",
    "import numpy as np\n",
    "from polars import DataFrame, col\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from label_legends.deberta_majority import load_dataset, load_deberta, load_dataset\n",
    "from label_legends.deberta import load_dataset, load_deberta, load_dataset\n",
    "from label_legends.preprocess import holdout, load_data, load_test, transform\n",
    "from label_legends.female import predict_female\n",
    "from label_legends.result import calculate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 6)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id    â”† text               â”† tokens             â”† token_ids          â”† label â”† rewire_id         â”‚\n",
      "â”‚ ---   â”† ---                â”† ---                â”† ---                â”† ---   â”† ---               â”‚\n",
      "â”‚ i64   â”† str                â”† list[str]          â”† list[i64]          â”† i64   â”† str               â”‚\n",
      "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0     â”† I wonder what      â”† [\"wonder\",         â”† [2942, 2935, â€¦     â”† 1     â”† sexism2022_englis â”‚\n",
      "â”‚       â”† keeps that witcâ€¦   â”† \"witch\", â€¦ \"ğŸ˜„\"]   â”† 3000]              â”†       â”† h-0               â”‚\n",
      "â”‚ 1     â”† I wonder what      â”† [\"wonder\",         â”† [2942, 2935, â€¦     â”† 1     â”† sexism2022_englis â”‚\n",
      "â”‚       â”† keeps that witcâ€¦   â”† \"witch\", â€¦ \"ğŸ˜„\"]   â”† 3000]              â”†       â”† h-0               â”‚\n",
      "â”‚ 10    â”† Good for her! My   â”† [\"good\", \"!\", â€¦    â”† [1181, 0, â€¦ 0]     â”† 0     â”† sexism2022_englis â”‚\n",
      "â”‚       â”† grandson had â€¦     â”† \"!\"]               â”†                    â”†       â”† h-100             â”‚\n",
      "â”‚ 1000  â”† The article said   â”† [\"article\", \"say\", â”† [245, 2331, â€¦ 26]  â”† 1     â”† sexism2022_englis â”‚\n",
      "â”‚       â”† Madonna offerâ€¦     â”† â€¦ \".\"]             â”†                    â”†       â”† h-10297           â”‚\n",
      "â”‚ 10000 â”† Gasoline. The      â”† [\"gasoline\", \".\",  â”† [3000, 26, â€¦ 26]   â”† 0     â”† sexism2022_englis â”‚\n",
      "â”‚       â”† answer 60% of Amâ€¦  â”† â€¦ \".\"]             â”†                    â”†       â”† h-12998           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4_200, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>rewire_id_number</th><th>label_sum</th><th>text</th><th>tokens</th><th>token_ids</th><th>label</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>list[str]</td><td>list[i64]</td><td>i32</td></tr></thead><tbody><tr><td>17886</td><td>0</td><td>&quot;How do you make a Quija Board â€¦</td><td>[&quot;make&quot;, &quot;quija&quot;, â€¦ &quot;.&quot;]</td><td>[1648, 3000, â€¦ 26]</td><td>0</td></tr><tr><td>7045</td><td>0</td><td>&quot;watch what women do not what tâ€¦</td><td>[&quot;watch&quot;, &quot;woman&quot;, &quot;say&quot;]</td><td>[2889, 2939, 2331]</td><td>0</td></tr><tr><td>18448</td><td>1</td><td>&quot;A certain Muppet female pig haâ€¦</td><td>[&quot;certain&quot;, &quot;muppet&quot;, â€¦ &quot;#metoo&quot;]</td><td>[508, 3000, â€¦ 3000]</td><td>0</td></tr><tr><td>18749</td><td>0</td><td>&quot;Katy Perry and her connection â€¦</td><td>[&quot;katy&quot;, &quot;perry&quot;, â€¦ &quot;.&quot;]</td><td>[3000, 3000, â€¦ 26]</td><td>0</td></tr><tr><td>15807</td><td>0</td><td>&quot; You got me! I&#x27;m really Soros&#x27;â€¦</td><td>[&quot;!&quot;, &quot;really&quot;, â€¦ &quot;blmforcash&quot;]</td><td>[0, 2194, â€¦ 3000]</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>10674</td><td>0</td><td>&quot;Block/delete her from your phoâ€¦</td><td>[&quot;block&quot;, &quot;/&quot;, â€¦ &quot;.&quot;]</td><td>[376, 35, â€¦ 26]</td><td>0</td></tr><tr><td>9724</td><td>3</td><td>&quot;These are accomplished women oâ€¦</td><td>[&quot;accomplish&quot;, &quot;woman&quot;, â€¦ &quot;.&quot;]</td><td>[124, 2939, â€¦ 26]</td><td>1</td></tr><tr><td>8234</td><td>0</td><td>&quot;Word of The Hour catastrophe: â€¦</td><td>[&quot;word&quot;, &quot;hour&quot;, â€¦ &quot;catÃ¡strofe&quot;]</td><td>[2945, 1318, â€¦ 3000]</td><td>0</td></tr><tr><td>2843</td><td>0</td><td>&quot;I&#x27;ll take your word for that. â€¦</td><td>[&quot;word&quot;, &quot;.&quot;, â€¦ &quot;....&quot;]</td><td>[2945, 26, â€¦ 29]</td><td>0</td></tr><tr><td>2924</td><td>1</td><td>&quot;This woman is an idiot and wasâ€¦</td><td>[&quot;woman&quot;, &quot;idiot&quot;, â€¦ &quot;.&quot;]</td><td>[2939, 1343, â€¦ 26]</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4_200, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ rewire_id_number â”† label_sum â”† text              â”† tokens            â”† token_ids         â”† label â”‚\n",
       "â”‚ ---              â”† ---       â”† ---               â”† ---               â”† ---               â”† ---   â”‚\n",
       "â”‚ i64              â”† i64       â”† str               â”† list[str]         â”† list[i64]         â”† i32   â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ 17886            â”† 0         â”† How do you make a â”† [\"make\", \"quija\", â”† [1648, 3000, â€¦    â”† 0     â”‚\n",
       "â”‚                  â”†           â”† Quija Board â€¦     â”† â€¦ \".\"]            â”† 26]               â”†       â”‚\n",
       "â”‚ 7045             â”† 0         â”† watch what women  â”† [\"watch\",         â”† [2889, 2939,      â”† 0     â”‚\n",
       "â”‚                  â”†           â”† do not what tâ€¦    â”† \"woman\", \"say\"]   â”† 2331]             â”†       â”‚\n",
       "â”‚ 18448            â”† 1         â”† A certain Muppet  â”† [\"certain\",       â”† [508, 3000, â€¦     â”† 0     â”‚\n",
       "â”‚                  â”†           â”† female pig haâ€¦    â”† \"muppet\", â€¦       â”† 3000]             â”†       â”‚\n",
       "â”‚                  â”†           â”†                   â”† \"#metoâ€¦           â”†                   â”†       â”‚\n",
       "â”‚ 18749            â”† 0         â”† Katy Perry and    â”† [\"katy\", \"perry\", â”† [3000, 3000, â€¦    â”† 0     â”‚\n",
       "â”‚                  â”†           â”† her connection â€¦  â”† â€¦ \".\"]            â”† 26]               â”†       â”‚\n",
       "â”‚ 15807            â”† 0         â”† You got me! I'm   â”† [\"!\", \"really\", â€¦ â”† [0, 2194, â€¦ 3000] â”† 0     â”‚\n",
       "â”‚                  â”†           â”† really Soros'â€¦    â”† \"blmforcash\"â€¦     â”†                   â”†       â”‚\n",
       "â”‚ â€¦                â”† â€¦         â”† â€¦                 â”† â€¦                 â”† â€¦                 â”† â€¦     â”‚\n",
       "â”‚ 10674            â”† 0         â”† Block/delete her  â”† [\"block\", \"/\", â€¦  â”† [376, 35, â€¦ 26]   â”† 0     â”‚\n",
       "â”‚                  â”†           â”† from your phoâ€¦    â”† \".\"]              â”†                   â”†       â”‚\n",
       "â”‚ 9724             â”† 3         â”† These are         â”† [\"accomplish\",    â”† [124, 2939, â€¦ 26] â”† 1     â”‚\n",
       "â”‚                  â”†           â”† accomplished      â”† \"woman\", â€¦ \".\"]   â”†                   â”†       â”‚\n",
       "â”‚                  â”†           â”† women oâ€¦          â”†                   â”†                   â”†       â”‚\n",
       "â”‚ 8234             â”† 0         â”† Word of The Hour  â”† [\"word\", \"hour\",  â”† [2945, 1318, â€¦    â”† 0     â”‚\n",
       "â”‚                  â”†           â”† catastrophe: â€¦    â”† â€¦ \"catÃ¡strofeâ€¦    â”† 3000]             â”†       â”‚\n",
       "â”‚ 2843             â”† 0         â”† I'll take your    â”† [\"word\", \".\", â€¦   â”† [2945, 26, â€¦ 29]  â”† 0     â”‚\n",
       "â”‚                  â”†           â”† word for that. â€¦  â”† \"....\"]           â”†                   â”†       â”‚\n",
       "â”‚ 2924             â”† 1         â”† This woman is an  â”† [\"woman\",         â”† [2939, 1343, â€¦    â”† 0     â”‚\n",
       "â”‚                  â”†           â”† idiot and wasâ€¦    â”† \"idiot\", â€¦ \".\"]   â”† 26]               â”†       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val, tra = holdout_majority()\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9_800, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>rewire_id_number</th><th>label_sum</th><th>text</th><th>tokens</th><th>token_ids</th><th>label</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>list[str]</td><td>list[i64]</td><td>i32</td></tr></thead><tbody><tr><td>8527</td><td>0</td><td>&quot;This is just perfect! I love tâ€¦</td><td>[&quot;just&quot;, &quot;perfect&quot;, â€¦ &quot;!&quot;]</td><td>[1483, 1967, â€¦ 0]</td><td>0</td></tr><tr><td>11951</td><td>1</td><td>&quot;She is good, at least she is nâ€¦</td><td>[&quot;good&quot;, &quot;,&quot;, â€¦ &quot;.&quot;]</td><td>[1181, 22, â€¦ 26]</td><td>0</td></tr><tr><td>14806</td><td>1</td><td>&quot;But of course, somehow the blaâ€¦</td><td>[&quot;course&quot;, &quot;,&quot;, â€¦ &quot;.&quot;]</td><td>[682, 22, â€¦ 26]</td><td>0</td></tr><tr><td>8817</td><td>0</td><td>&quot;Idk man, maybe you leave her sâ€¦</td><td>[&quot;idk&quot;, &quot;man&quot;, â€¦ &quot;reply&quot;]</td><td>[1345, 1652, â€¦ 2238]</td><td>0</td></tr><tr><td>17475</td><td>3</td><td>&quot;back again hi I am sammay I liâ€¦</td><td>[&quot;hi&quot;, &quot;sammay&quot;, â€¦ &quot;pest&quot;]</td><td>[1272, 3000, â€¦ 3000]</td><td>1</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>18316</td><td>2</td><td>&quot;Hard of hearing chicks are nicâ€¦</td><td>[&quot;hard&quot;, &quot;hearing&quot;, â€¦ &quot;.&quot;]</td><td>[1243, 3000, â€¦ 26]</td><td>1</td></tr><tr><td>6983</td><td>3</td><td>&quot;This actually makes me angry.Iâ€¦</td><td>[&quot;actually&quot;, &quot;make&quot;, â€¦ &quot;case&quot;]</td><td>[143, 1648, â€¦ 493]</td><td>1</td></tr><tr><td>17348</td><td>3</td><td>&quot;the problem with this is that â€¦</td><td>[&quot;problem&quot;, &quot;cuck&quot;, â€¦ &quot;?&quot;]</td><td>[2091, 712, â€¦ 98]</td><td>1</td></tr><tr><td>3666</td><td>3</td><td>&quot;This is bollocks! You can be wâ€¦</td><td>[&quot;bollock&quot;, &quot;!&quot;, â€¦ &quot;receive&quot;]</td><td>[3000, 0, â€¦ 2198]</td><td>1</td></tr><tr><td>19773</td><td>0</td><td>&quot;Look.....just because you haveâ€¦</td><td>[&quot;look&quot;, &quot;.....&quot;, â€¦ &quot;.&quot;]</td><td>[1608, 30, â€¦ 26]</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9_800, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ rewire_id_number â”† label_sum â”† text              â”† tokens            â”† token_ids         â”† label â”‚\n",
       "â”‚ ---              â”† ---       â”† ---               â”† ---               â”† ---               â”† ---   â”‚\n",
       "â”‚ i64              â”† i64       â”† str               â”† list[str]         â”† list[i64]         â”† i32   â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ 8527             â”† 0         â”† This is just      â”† [\"just\",          â”† [1483, 1967, â€¦ 0] â”† 0     â”‚\n",
       "â”‚                  â”†           â”† perfect! I love   â”† \"perfect\", â€¦ \"!\"] â”†                   â”†       â”‚\n",
       "â”‚                  â”†           â”† tâ€¦                â”†                   â”†                   â”†       â”‚\n",
       "â”‚ 11951            â”† 1         â”† She is good, at   â”† [\"good\", \",\", â€¦   â”† [1181, 22, â€¦ 26]  â”† 0     â”‚\n",
       "â”‚                  â”†           â”† least she is nâ€¦   â”† \".\"]              â”†                   â”†       â”‚\n",
       "â”‚ 14806            â”† 1         â”† But of course,    â”† [\"course\", \",\", â€¦ â”† [682, 22, â€¦ 26]   â”† 0     â”‚\n",
       "â”‚                  â”†           â”† somehow the blaâ€¦  â”† \".\"]              â”†                   â”†       â”‚\n",
       "â”‚ 8817             â”† 0         â”† Idk man, maybe    â”† [\"idk\", \"man\", â€¦  â”† [1345, 1652, â€¦    â”† 0     â”‚\n",
       "â”‚                  â”†           â”† you leave her sâ€¦  â”† \"reply\"]          â”† 2238]             â”†       â”‚\n",
       "â”‚ 17475            â”† 3         â”† back again hi I   â”† [\"hi\", \"sammay\",  â”† [1272, 3000, â€¦    â”† 1     â”‚\n",
       "â”‚                  â”†           â”† am sammay I liâ€¦   â”† â€¦ \"pest\"]         â”† 3000]             â”†       â”‚\n",
       "â”‚ â€¦                â”† â€¦         â”† â€¦                 â”† â€¦                 â”† â€¦                 â”† â€¦     â”‚\n",
       "â”‚ 18316            â”† 2         â”† Hard of hearing   â”† [\"hard\",          â”† [1243, 3000, â€¦    â”† 1     â”‚\n",
       "â”‚                  â”†           â”† chicks are nicâ€¦   â”† \"hearing\", â€¦ \".\"] â”† 26]               â”†       â”‚\n",
       "â”‚ 6983             â”† 3         â”† This actually     â”† [\"actually\",      â”† [143, 1648, â€¦     â”† 1     â”‚\n",
       "â”‚                  â”†           â”† makes me angry.Iâ€¦ â”† \"make\", â€¦ \"case\"] â”† 493]              â”†       â”‚\n",
       "â”‚ 17348            â”† 3         â”† the problem with  â”† [\"problem\",       â”† [2091, 712, â€¦ 98] â”† 1     â”‚\n",
       "â”‚                  â”†           â”† this is that â€¦    â”† \"cuck\", â€¦ \"?\"]    â”†                   â”†       â”‚\n",
       "â”‚ 3666             â”† 3         â”† This is bollocks! â”† [\"bollock\", \"!\",  â”† [3000, 0, â€¦ 2198] â”† 1     â”‚\n",
       "â”‚                  â”†           â”† You can be wâ€¦     â”† â€¦ \"receive\"]      â”†                   â”†       â”‚\n",
       "â”‚ 19773            â”† 0         â”† Look.....just     â”† [\"look\", \".....\", â”† [1608, 30, â€¦ 26]  â”† 0     â”‚\n",
       "â”‚                  â”†           â”† because you haveâ€¦ â”† â€¦ \".\"]            â”†                   â”†       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deberta running on majority here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "deberta = load_deberta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3065 [00:00<?, ?it/s]/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/src/label_legends/deberta_majority.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "  3%|â–         | 100/3065 [00:58<30:21,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5901, 'grad_norm': 0.8738930821418762, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 200/3065 [01:55<27:07,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5517, 'grad_norm': 2.5674209594726562, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 300/3065 [02:51<25:58,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4724, 'grad_norm': 5.353095531463623, 'learning_rate': 1.9644760213143874e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–        | 400/3065 [03:49<25:36,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3738, 'grad_norm': 4.014645099639893, 'learning_rate': 1.8934280639431617e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–‹        | 500/3065 [04:46<24:29,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3817, 'grad_norm': 4.943447113037109, 'learning_rate': 1.822380106571936e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 600/3065 [05:44<23:32,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3499, 'grad_norm': 1.85252845287323, 'learning_rate': 1.751332149200711e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–ˆ        | 613/3065 [06:36<25:46,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31990790367126465, 'eval_precision': 0.779601406799531, 'eval_recall': 0.646887159533074, 'eval_fscore': 0.7070707070707071, 'eval_accuracy': 0.8688095238095238, 'eval_tp': 665, 'eval_tn': 2984, 'eval_fp': 188, 'eval_fn': 363, 'eval_runtime': 45.1225, 'eval_samples_per_second': 93.08, 'eval_steps_per_second': 5.829, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/src/label_legends/deberta_majority.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 23%|â–ˆâ–ˆâ–       | 700/3065 [07:29<22:45,  1.73it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3231, 'grad_norm': 1.4562348127365112, 'learning_rate': 1.680284191829485e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 800/3065 [08:26<21:50,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2993, 'grad_norm': 2.7944037914276123, 'learning_rate': 1.6092362344582596e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 900/3065 [09:24<20:40,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2803, 'grad_norm': 3.1831862926483154, 'learning_rate': 1.5381882770870337e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 1000/3065 [10:22<19:43,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2828, 'grad_norm': 10.380820274353027, 'learning_rate': 1.4671403197158082e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1100/3065 [11:19<18:51,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2972, 'grad_norm': 12.751142501831055, 'learning_rate': 1.3960923623445828e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1200/3065 [12:16<18:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3006, 'grad_norm': 3.834257125854492, 'learning_rate': 1.3250444049733571e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1226/3065 [13:17<16:25,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34876400232315063, 'eval_precision': 0.7070524412296564, 'eval_recall': 0.7607003891050583, 'eval_fscore': 0.7328959700093721, 'eval_accuracy': 0.8642857142857143, 'eval_tp': 782, 'eval_tn': 2848, 'eval_fp': 324, 'eval_fn': 246, 'eval_runtime': 45.4507, 'eval_samples_per_second': 92.408, 'eval_steps_per_second': 5.786, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/src/label_legends/deberta_majority.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1300/3065 [14:02<16:45,  1.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2079, 'grad_norm': 12.443868637084961, 'learning_rate': 1.2539964476021315e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1400/3065 [14:59<15:55,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2114, 'grad_norm': 3.08478045463562, 'learning_rate': 1.1829484902309059e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1500/3065 [15:57<15:03,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1975, 'grad_norm': 6.4972758293151855, 'learning_rate': 1.1119005328596803e-05, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1600/3065 [16:54<14:06,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1956, 'grad_norm': 6.872176170349121, 'learning_rate': 1.0408525754884548e-05, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1700/3065 [17:51<13:04,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1776, 'grad_norm': 20.328386306762695, 'learning_rate': 9.698046181172292e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1800/3065 [18:48<11:56,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1975, 'grad_norm': 10.59494400024414, 'learning_rate': 8.987566607460036e-06, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1839/3065 [19:56<11:10,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45301249623298645, 'eval_precision': 0.7348119575699132, 'eval_recall': 0.7412451361867705, 'eval_fscore': 0.7380145278450363, 'eval_accuracy': 0.8711904761904762, 'eval_tp': 762, 'eval_tn': 2897, 'eval_fp': 275, 'eval_fn': 266, 'eval_runtime': 44.9441, 'eval_samples_per_second': 93.45, 'eval_steps_per_second': 5.852, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/src/label_legends/deberta_majority.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1900/3065 [20:33<11:00,  1.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1531, 'grad_norm': 10.173916816711426, 'learning_rate': 8.27708703374778e-06, 'epoch': 3.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2000/3065 [21:30<10:10,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1471, 'grad_norm': 6.339789867401123, 'learning_rate': 7.566607460035525e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2100/3065 [22:27<09:06,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1082, 'grad_norm': 0.07047244906425476, 'learning_rate': 6.8561278863232685e-06, 'epoch': 3.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2200/3065 [23:24<08:09,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1362, 'grad_norm': 0.5132527351379395, 'learning_rate': 6.145648312611013e-06, 'epoch': 3.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2300/3065 [24:21<07:30,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1175, 'grad_norm': 0.686800479888916, 'learning_rate': 5.435168738898757e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2400/3065 [25:19<06:19,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1266, 'grad_norm': 9.269302368164062, 'learning_rate': 4.724689165186501e-06, 'epoch': 3.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2452/3065 [26:33<05:20,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5708104372024536, 'eval_precision': 0.7416413373860182, 'eval_recall': 0.7120622568093385, 'eval_fscore': 0.7265508684863523, 'eval_accuracy': 0.8688095238095238, 'eval_tp': 732, 'eval_tn': 2917, 'eval_fp': 255, 'eval_fn': 296, 'eval_runtime': 45.1898, 'eval_samples_per_second': 92.941, 'eval_steps_per_second': 5.82, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/src/label_legends/deberta_majority.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2500/3065 [27:03<05:22,  1.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1053, 'grad_norm': 0.9246652126312256, 'learning_rate': 4.0142095914742455e-06, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2600/3065 [28:00<04:25,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0718, 'grad_norm': 0.13763460516929626, 'learning_rate': 3.3037300177619897e-06, 'epoch': 4.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2700/3065 [28:58<03:27,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0744, 'grad_norm': 0.2980663776397705, 'learning_rate': 2.5932504440497336e-06, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2800/3065 [29:55<02:30,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1005, 'grad_norm': 28.969125747680664, 'learning_rate': 1.882770870337478e-06, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2900/3065 [30:52<01:33,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0785, 'grad_norm': 2.0508663654327393, 'learning_rate': 1.172291296625222e-06, 'epoch': 4.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3000/3065 [31:49<00:37,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0903, 'grad_norm': 0.09931075572967529, 'learning_rate': 4.618117229129663e-07, 'epoch': 4.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3065/3065 [32:26<00:00,  1.88it/s]/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/src/label_legends/deberta_majority.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3065/3065 [33:13<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6569569110870361, 'eval_precision': 0.7240038872691934, 'eval_recall': 0.7247081712062257, 'eval_fscore': 0.7243558580456976, 'eval_accuracy': 0.865, 'eval_tp': 745, 'eval_tn': 2888, 'eval_fp': 284, 'eval_fn': 283, 'eval_runtime': 45.3808, 'eval_samples_per_second': 92.55, 'eval_steps_per_second': 5.795, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3065/3065 [33:17<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1997.2714, 'train_samples_per_second': 24.533, 'train_steps_per_second': 1.535, 'train_loss': 0.23006841447963994, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3065/3065 [33:18<00:00,  1.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 1997.2714,\n",
       " 'train_samples_per_second': 24.533,\n",
       " 'train_steps_per_second': 1.535,\n",
       " 'total_flos': 2241109160556000.0,\n",
       " 'train_loss': 0.23006841447963994,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_out = deberta.train()\n",
    "train_out.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/retipeter/Documents/TU Wien/NLP/nlp-ie-label-legends/src/label_legends/deberta_majority.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [02:05<00:00,  5.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.02157354, -0.15974028],\n",
       "       [ 0.16012895, -0.31555876],\n",
       "       [ 0.16012895, -0.31555876],\n",
       "       ...,\n",
       "       [-0.03087621, -0.13163614],\n",
       "       [-0.03087621, -0.13163614],\n",
       "       [-0.03087621, -0.13163614]], dtype=float32), label_ids=array([1, 0, 0, ..., 1, 0, 1]), metrics={'test_loss': 0.38887882232666016, 'test_precision': 0.7482574825748257, 'test_recall': 0.5858747993579454, 'test_fscore': 0.6571840115232265, 'test_accuracy': 0.8413333333333334, 'test_tp': 1825, 'test_tn': 8271, 'test_fp': 614, 'test_fn': 1290, 'test_runtime': 126.3444, 'test_samples_per_second': 94.978, 'test_steps_per_second': 5.936})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = load_test().collect()\n",
    "test_transformed = transform(test)\n",
    "deberta_prediction = deberta.predict(load_dataset(test_transformed[\"text\"].to_list(), test_transformed[\"label\"].to_list()))\n",
    "deberta_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>label</th><th>deberta</th></tr><tr><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>100</td><td>1</td><td>0</td></tr><tr><td>10005</td><td>0</td><td>0</td></tr><tr><td>10006</td><td>0</td><td>0</td></tr><tr><td>10007</td><td>0</td><td>0</td></tr><tr><td>10008</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ id    â”† label â”† deberta â”‚\n",
       "â”‚ ---   â”† ---   â”† ---     â”‚\n",
       "â”‚ i64   â”† i64   â”† i64     â”‚\n",
       "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 100   â”† 1     â”† 0       â”‚\n",
       "â”‚ 10005 â”† 0     â”† 0       â”‚\n",
       "â”‚ 10006 â”† 0     â”† 0       â”‚\n",
       "â”‚ 10007 â”† 0     â”† 0       â”‚\n",
       "â”‚ 10008 â”† 0     â”† 0       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = DataFrame({\"id\": test[\"id\"], \"label\": deberta_prediction.label_ids, \"deberta\": np.argmax(deberta_prediction.predictions, axis=1)})\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision:\t0.7483\n",
       "recall:\t\t0.5859\n",
       "fscore:\t\t0.6572\n",
       "accuracy:\t0.8413\n",
       "tn: 8271\t fp: 614\n",
       "fn: 1290\t tp: 1825"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_deberta = calculate_scores(predictions[\"label\"], predictions[\"deberta\"])\n",
    "scores_deberta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
