{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4818c36",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a57fa4d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The autoreload extension is already loaded. To reload it, use:\n",
       "  %reload_ext autoreload\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c14631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import TokenList\n",
    "import polars\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import stanza\n",
    "from stanza.models.common.doc import Token\n",
    "from label_legends.preprocess import ConlluTokenizer, analyzer, create_conllu, holdout, load_conllu, load_data, load_train, transform, load_vectorizer, reverse_vocabulary, vocabulary, ids_to_tokens, tokens_to_ids, vectorize_tokens, strip_stopwords\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b545de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to create conllu files once\n",
    "# create_conllu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "503fb210",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (60_000, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>rewire_id</th><th>text</th><th>annotator</th><th>label_sexist</th><th>label_category</th><th>label_vector</th><th>split</th><th>tokens</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[str]</td></tr></thead><tbody><tr><td>0</td><td>&quot;sexism2022_english-0&quot;</td><td>&quot; I wonder what keeps that witc…</td><td>17</td><td>&quot;sexist&quot;</td><td>&quot;2. derogation&quot;</td><td>&quot;2.2 aggressive and emotive att…</td><td>&quot;train&quot;</td><td>[&quot;i&quot;, &quot;wonder&quot;, … &quot;😄&quot;]</td></tr><tr><td>1</td><td>&quot;sexism2022_english-0&quot;</td><td>&quot; I wonder what keeps that witc…</td><td>2</td><td>&quot;sexist&quot;</td><td>&quot;2. derogation&quot;</td><td>&quot;2.2 aggressive and emotive att…</td><td>&quot;train&quot;</td><td>[&quot;i&quot;, &quot;wonder&quot;, … &quot;😄&quot;]</td></tr><tr><td>10</td><td>&quot;sexism2022_english-100&quot;</td><td>&quot;Good for her! My grandson had …</td><td>3</td><td>&quot;not sexist&quot;</td><td>&quot;none&quot;</td><td>&quot;none&quot;</td><td>&quot;train&quot;</td><td>[&quot;good&quot;, &quot;for&quot;, … &quot;!&quot;]</td></tr><tr><td>100</td><td>&quot;sexism2022_english-10026&quot;</td><td>&quot;It is not insulting, it&#x27;s **ex…</td><td>14</td><td>&quot;sexist&quot;</td><td>&quot;2. derogation&quot;</td><td>&quot;2.1 descriptive attacks&quot;</td><td>&quot;test&quot;</td><td>[&quot;it&quot;, &quot;be&quot;, … &quot;.**&quot;]</td></tr><tr><td>1000</td><td>&quot;sexism2022_english-10297&quot;</td><td>&quot;The article said Madonna offer…</td><td>5</td><td>&quot;sexist&quot;</td><td>&quot;2. derogation&quot;</td><td>&quot;2.3 dehumanising attacks &amp; ove…</td><td>&quot;train&quot;</td><td>[&quot;the&quot;, &quot;article&quot;, … &quot;.&quot;]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>9995</td><td>&quot;sexism2022_english-12996&quot;</td><td>&quot;Shudder.. if you had to have s…</td><td>17</td><td>&quot;sexist&quot;</td><td>&quot;2. derogation&quot;</td><td>&quot;2.3 dehumanising attacks &amp; ove…</td><td>&quot;test&quot;</td><td>[&quot;shudder&quot;, &quot;..&quot;, … &quot;.&quot;]</td></tr><tr><td>9996</td><td>&quot;sexism2022_english-12997&quot;</td><td>&quot;You mean one that forces women…</td><td>6</td><td>&quot;not sexist&quot;</td><td>&quot;none&quot;</td><td>&quot;none&quot;</td><td>&quot;train&quot;</td><td>[&quot;you&quot;, &quot;mean&quot;, … &quot;?&quot;]</td></tr><tr><td>9997</td><td>&quot;sexism2022_english-12997&quot;</td><td>&quot;You mean one that forces women…</td><td>4</td><td>&quot;not sexist&quot;</td><td>&quot;none&quot;</td><td>&quot;none&quot;</td><td>&quot;train&quot;</td><td>[&quot;you&quot;, &quot;mean&quot;, … &quot;?&quot;]</td></tr><tr><td>9998</td><td>&quot;sexism2022_english-12997&quot;</td><td>&quot;You mean one that forces women…</td><td>2</td><td>&quot;sexist&quot;</td><td>&quot;3. animosity&quot;</td><td>&quot;3.2 immutable gender differenc…</td><td>&quot;train&quot;</td><td>[&quot;you&quot;, &quot;mean&quot;, … &quot;?&quot;]</td></tr><tr><td>9999</td><td>&quot;sexism2022_english-12998&quot;</td><td>&quot;Gasoline. The answer 60% of Am…</td><td>9</td><td>&quot;sexist&quot;</td><td>&quot;1. threats, plans to harm and …</td><td>&quot;1.1 threats of harm&quot;</td><td>&quot;train&quot;</td><td>[&quot;gasoline&quot;, &quot;.&quot;, … &quot;.&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (60_000, 9)\n",
       "┌──────┬─────────────┬─────────────┬───────────┬───┬─────────────┬────────────┬───────┬────────────┐\n",
       "│ id   ┆ rewire_id   ┆ text        ┆ annotator ┆ … ┆ label_categ ┆ label_vect ┆ split ┆ tokens     │\n",
       "│ ---  ┆ ---         ┆ ---         ┆ ---       ┆   ┆ ory         ┆ or         ┆ ---   ┆ ---        │\n",
       "│ i64  ┆ str         ┆ str         ┆ i64       ┆   ┆ ---         ┆ ---        ┆ str   ┆ list[str]  │\n",
       "│      ┆             ┆             ┆           ┆   ┆ str         ┆ str        ┆       ┆            │\n",
       "╞══════╪═════════════╪═════════════╪═══════════╪═══╪═════════════╪════════════╪═══════╪════════════╡\n",
       "│ 0    ┆ sexism2022_ ┆ I wonder    ┆ 17        ┆ … ┆ 2.          ┆ 2.2        ┆ train ┆ [\"i\",      │\n",
       "│      ┆ english-0   ┆ what keeps  ┆           ┆   ┆ derogation  ┆ aggressive ┆       ┆ \"wonder\",  │\n",
       "│      ┆             ┆ that witc…  ┆           ┆   ┆             ┆ and        ┆       ┆ … \"😄\"]    │\n",
       "│      ┆             ┆             ┆           ┆   ┆             ┆ emotive    ┆       ┆            │\n",
       "│      ┆             ┆             ┆           ┆   ┆             ┆ att…       ┆       ┆            │\n",
       "│ 1    ┆ sexism2022_ ┆ I wonder    ┆ 2         ┆ … ┆ 2.          ┆ 2.2        ┆ train ┆ [\"i\",      │\n",
       "│      ┆ english-0   ┆ what keeps  ┆           ┆   ┆ derogation  ┆ aggressive ┆       ┆ \"wonder\",  │\n",
       "│      ┆             ┆ that witc…  ┆           ┆   ┆             ┆ and        ┆       ┆ … \"😄\"]    │\n",
       "│      ┆             ┆             ┆           ┆   ┆             ┆ emotive    ┆       ┆            │\n",
       "│      ┆             ┆             ┆           ┆   ┆             ┆ att…       ┆       ┆            │\n",
       "│ 10   ┆ sexism2022_ ┆ Good for    ┆ 3         ┆ … ┆ none        ┆ none       ┆ train ┆ [\"good\",   │\n",
       "│      ┆ english-100 ┆ her! My     ┆           ┆   ┆             ┆            ┆       ┆ \"for\", …   │\n",
       "│      ┆             ┆ grandson    ┆           ┆   ┆             ┆            ┆       ┆ \"!\"]       │\n",
       "│      ┆             ┆ had …       ┆           ┆   ┆             ┆            ┆       ┆            │\n",
       "│ 100  ┆ sexism2022_ ┆ It is not   ┆ 14        ┆ … ┆ 2.          ┆ 2.1 descri ┆ test  ┆ [\"it\",     │\n",
       "│      ┆ english-100 ┆ insulting,  ┆           ┆   ┆ derogation  ┆ ptive      ┆       ┆ \"be\", …    │\n",
       "│      ┆ 26          ┆ it's **ex…  ┆           ┆   ┆             ┆ attacks    ┆       ┆ \".**\"]     │\n",
       "│ 1000 ┆ sexism2022_ ┆ The article ┆ 5         ┆ … ┆ 2.          ┆ 2.3 dehuma ┆ train ┆ [\"the\",    │\n",
       "│      ┆ english-102 ┆ said        ┆           ┆   ┆ derogation  ┆ nising     ┆       ┆ \"article\", │\n",
       "│      ┆ 97          ┆ Madonna     ┆           ┆   ┆             ┆ attacks &  ┆       ┆ … \".\"]     │\n",
       "│      ┆             ┆ offer…      ┆           ┆   ┆             ┆ ove…       ┆       ┆            │\n",
       "│ …    ┆ …           ┆ …           ┆ …         ┆ … ┆ …           ┆ …          ┆ …     ┆ …          │\n",
       "│ 9995 ┆ sexism2022_ ┆ Shudder..   ┆ 17        ┆ … ┆ 2.          ┆ 2.3 dehuma ┆ test  ┆ [\"shudder\" │\n",
       "│      ┆ english-129 ┆ if you had  ┆           ┆   ┆ derogation  ┆ nising     ┆       ┆ , \"..\", …  │\n",
       "│      ┆ 96          ┆ to have s…  ┆           ┆   ┆             ┆ attacks &  ┆       ┆ \".\"]       │\n",
       "│      ┆             ┆             ┆           ┆   ┆             ┆ ove…       ┆       ┆            │\n",
       "│ 9996 ┆ sexism2022_ ┆ You mean    ┆ 6         ┆ … ┆ none        ┆ none       ┆ train ┆ [\"you\",    │\n",
       "│      ┆ english-129 ┆ one that    ┆           ┆   ┆             ┆            ┆       ┆ \"mean\", …  │\n",
       "│      ┆ 97          ┆ forces      ┆           ┆   ┆             ┆            ┆       ┆ \"?\"]       │\n",
       "│      ┆             ┆ women…      ┆           ┆   ┆             ┆            ┆       ┆            │\n",
       "│ 9997 ┆ sexism2022_ ┆ You mean    ┆ 4         ┆ … ┆ none        ┆ none       ┆ train ┆ [\"you\",    │\n",
       "│      ┆ english-129 ┆ one that    ┆           ┆   ┆             ┆            ┆       ┆ \"mean\", …  │\n",
       "│      ┆ 97          ┆ forces      ┆           ┆   ┆             ┆            ┆       ┆ \"?\"]       │\n",
       "│      ┆             ┆ women…      ┆           ┆   ┆             ┆            ┆       ┆            │\n",
       "│ 9998 ┆ sexism2022_ ┆ You mean    ┆ 2         ┆ … ┆ 3.          ┆ 3.2        ┆ train ┆ [\"you\",    │\n",
       "│      ┆ english-129 ┆ one that    ┆           ┆   ┆ animosity   ┆ immutable  ┆       ┆ \"mean\", …  │\n",
       "│      ┆ 97          ┆ forces      ┆           ┆   ┆             ┆ gender     ┆       ┆ \"?\"]       │\n",
       "│      ┆             ┆ women…      ┆           ┆   ┆             ┆ differenc… ┆       ┆            │\n",
       "│ 9999 ┆ sexism2022_ ┆ Gasoline.   ┆ 9         ┆ … ┆ 1. threats, ┆ 1.1        ┆ train ┆ [\"gasoline │\n",
       "│      ┆ english-129 ┆ The answer  ┆           ┆   ┆ plans to    ┆ threats of ┆       ┆ \", \".\", …  │\n",
       "│      ┆ 98          ┆ 60% of Am…  ┆           ┆   ┆ harm and …  ┆ harm       ┆       ┆ \".\"]       │\n",
       "└──────┴─────────────┴─────────────┴───────────┴───┴─────────────┴────────────┴───────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load_data().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8d9db",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79b8aa6f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/home/lukas/Programming/uni/nlp-ie-label-legends/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val, tra = holdout()\n",
    "tra = transform(tra)\n",
    "val = transform(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06678ad",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6994a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (29_400, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>text</th><th>tokens</th><th>token_ids</th><th>label</th></tr><tr><td>i64</td><td>str</td><td>list[str]</td><td>list[i64]</td><td>str</td></tr></thead><tbody><tr><td>26016</td><td>&quot;#VoteDemOut seditionist wanna …</td><td>[&quot;#&quot;, &quot;votedemout&quot;, … &quot;ven&quot;]</td><td>[7, 3000, … 3000]</td><td>&quot;0&quot;</td></tr><tr><td>41060</td><td>&quot;Cerno hops from wave to wave: …</td><td>[&quot;cerno&quot;, &quot;hop&quot;, … &quot;.&quot;]</td><td>[3000, 3000, … 26]</td><td>&quot;1&quot;</td></tr><tr><td>35766</td><td>&quot;think about it.. he´s called J…</td><td>[&quot;think&quot;, &quot;..&quot;, … &quot;elect&quot;]</td><td>[2688, 27, … 911]</td><td>&quot;0&quot;</td></tr><tr><td>23678</td><td>&quot;Hmm, you could rewrite this wi…</td><td>[&quot;hmm&quot;, &quot;,&quot;, … &quot;justsaying&quot;]</td><td>[1284, 22, … 3000]</td><td>&quot;0&quot;</td></tr><tr><td>52396</td><td>&quot;Every girl in a game, or on si…</td><td>[&quot;girl&quot;, &quot;game&quot;, … &quot;.&quot;]</td><td>[1164, 1135, … 26]</td><td>&quot;0&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>6785</td><td>&quot;I just think my dick couldn&#x27;t …</td><td>[&quot;just&quot;, &quot;think&quot;, … &quot;..😂😂😂&quot;]</td><td>[1482, 2688, … 3000]</td><td>&quot;0&quot;</td></tr><tr><td>31355</td><td>&quot;no he deadasss hit the nail on…</td><td>[&quot;deadass&quot;, &quot;hit&quot;, … &quot;.&quot;]</td><td>[3000, 1282, … 26]</td><td>&quot;0&quot;</td></tr><tr><td>49834</td><td>&quot;Aren&#x27;t these goblins always wh…</td><td>[&quot;goblins&quot;, &quot;whine&quot;, … &quot;hastagsarestillcool&quot;]</td><td>[3000, 2914, … 3000]</td><td>&quot;0&quot;</td></tr><tr><td>26158</td><td>&quot;1. Stop calling girls, women a…</td><td>[&quot;1.&quot;, &quot;stop&quot;, … &quot;.&quot;]</td><td>[39, 2567, … 26]</td><td>&quot;0&quot;</td></tr><tr><td>2905</td><td>&quot;Wives and nannies, but the fat…</td><td>[&quot;wife&quot;, &quot;nanny&quot;, … &quot;.&quot;]</td><td>[2920, 3000, … 26]</td><td>&quot;1&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (29_400, 5)\n",
       "┌───────┬─────────────────────────────┬─────────────────────────────┬──────────────────────┬───────┐\n",
       "│ id    ┆ text                        ┆ tokens                      ┆ token_ids            ┆ label │\n",
       "│ ---   ┆ ---                         ┆ ---                         ┆ ---                  ┆ ---   │\n",
       "│ i64   ┆ str                         ┆ list[str]                   ┆ list[i64]            ┆ str   │\n",
       "╞═══════╪═════════════════════════════╪═════════════════════════════╪══════════════════════╪═══════╡\n",
       "│ 26016 ┆ #VoteDemOut seditionist     ┆ [\"#\", \"votedemout\", …       ┆ [7, 3000, … 3000]    ┆ 0     │\n",
       "│       ┆ wanna …                     ┆ \"ven\"]                      ┆                      ┆       │\n",
       "│ 41060 ┆ Cerno hops from wave to     ┆ [\"cerno\", \"hop\", … \".\"]     ┆ [3000, 3000, … 26]   ┆ 1     │\n",
       "│       ┆ wave: …                     ┆                             ┆                      ┆       │\n",
       "│ 35766 ┆ think about it.. he´s       ┆ [\"think\", \"..\", … \"elect\"]  ┆ [2688, 27, … 911]    ┆ 0     │\n",
       "│       ┆ called J…                   ┆                             ┆                      ┆       │\n",
       "│ 23678 ┆ Hmm, you could rewrite this ┆ [\"hmm\", \",\", …              ┆ [1284, 22, … 3000]   ┆ 0     │\n",
       "│       ┆ wi…                         ┆ \"justsaying\"]               ┆                      ┆       │\n",
       "│ 52396 ┆ Every girl in a game, or on ┆ [\"girl\", \"game\", … \".\"]     ┆ [1164, 1135, … 26]   ┆ 0     │\n",
       "│       ┆ si…                         ┆                             ┆                      ┆       │\n",
       "│ …     ┆ …                           ┆ …                           ┆ …                    ┆ …     │\n",
       "│ 6785  ┆ I just think my dick        ┆ [\"just\", \"think\", …         ┆ [1482, 2688, … 3000] ┆ 0     │\n",
       "│       ┆ couldn't …                  ┆ \"..😂😂😂\"]                 ┆                      ┆       │\n",
       "│ 31355 ┆ no he deadasss hit the nail ┆ [\"deadass\", \"hit\", … \".\"]   ┆ [3000, 1282, … 26]   ┆ 0     │\n",
       "│       ┆ on…                         ┆                             ┆                      ┆       │\n",
       "│ 49834 ┆ Aren't these goblins always ┆ [\"goblins\", \"whine\", …      ┆ [3000, 2914, … 3000] ┆ 0     │\n",
       "│       ┆ wh…                         ┆ \"hastag…                    ┆                      ┆       │\n",
       "│ 26158 ┆ 1. Stop calling girls,      ┆ [\"1.\", \"stop\", … \".\"]       ┆ [39, 2567, … 26]     ┆ 0     │\n",
       "│       ┆ women a…                    ┆                             ┆                      ┆       │\n",
       "│ 2905  ┆ Wives and nannies, but the  ┆ [\"wife\", \"nanny\", … \".\"]    ┆ [2920, 3000, … 26]   ┆ 1     │\n",
       "│       ┆ fat…                        ┆                             ┆                      ┆       │\n",
       "└───────┴─────────────────────────────┴─────────────────────────────┴──────────────────────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tra[:10].select(\"tokens\").to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a76bba5b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtra\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# .toarray().nonzero()\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/uni/nlp-ie-label-legends/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1417\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1419\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/uni/nlp-ie-label-legends/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Programming/uni/nlp-ie-label-legends/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[0;32m~/Programming/uni/nlp-ie-label-legends/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:246\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[1;32m    249\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "File \u001b[0;32m~/Programming/uni/nlp-ie-label-legends/src/label_legends/preprocess.py:28\u001b[0m, in \u001b[0;36mConlluTokenizer.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     26\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstopwords\u001b[49m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "load_vectorizer().transform([[\"want\"], [\"want\"], [\"want\", \"want\", \"want\"]]).toarray().nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf815c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer()([\"want\", \"want\", \"want\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d840bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary()[\"want\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
